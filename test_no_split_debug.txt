================================================================================
Testing Optimized QKV Fusion Correctness
================================================================================
Input shape: torch.Size([2, 128, 2048])
Configuration: 32 Q heads, 4 KV heads, 128 head_dim

Q proj weight shape: torch.Size([4096, 2048])
K proj weight shape: torch.Size([512, 2048])
V proj weight shape: torch.Size([512, 2048])

[1] Running PyTorch baseline (3 nn.Linear)...
Q baseline shape: torch.Size([2, 32, 128, 128])
K baseline shape: torch.Size([2, 4, 128, 128])
V baseline shape: torch.Size([2, 4, 128, 128])

[2] Running optimized cuBLAS kernel...
Fused weight shape: torch.Size([2048, 5120])
Fused bias shape: torch.Size([5120])
DEBUG: qkv_output shape: torch.Size([2, 128, 5120])
DEBUG: Expected: [2, 128, 5120]
DEBUG: qkv_output stride: (655360, 5120, 1)
DEBUG: qkv_output is_contiguous: True
Q optimized shape: torch.Size([2, 32, 128, 128])
K optimized shape: torch.Size([2, 4, 128, 128])
V optimized shape: torch.Size([2, 4, 128, 128])

[3] Comparing results...
Q max absolute diff: 0.030273
K max absolute diff: 0.028320
V max absolute diff: 0.029297
Q mean relative error: 0.166870
K mean relative error: 0.150635
V mean relative error: 0.146729

âœ— FAIL: Results differ too much!

Debugging info:
  Q sample values (baseline): tensor([ 0.7944,  0.3223, -0.6289, -0.2759,  0.6577], device='cuda:0',
       dtype=torch.float16)
  Q sample values (optimized): tensor([ 0.8018,  0.3428, -0.6201, -0.2769,  0.6567], device='cuda:0',
       dtype=torch.float16)

Skipping benchmark due to correctness test failure.
